Scenario 1:
hdfs dfs -mkdir /salary
hdfs dfs -put Downloads/Bigdata/BigData_HESS/Evaluation_Test/sample-files-for-test/salarydet.txt /salary
hdfs dfs -ls /salary

#create table
hive> create table salarydata1 (employee_id int, employee_name string,bonus_percentage int,yop map<string,string>) 
    > row format delimited
    > fields terminated by ','
    > map keys terminated by ':';

#describe your table
hive>describe formatted salarydata1;
# col_name            	data_type           	comment             
	 	 
employee_id         	int                 	                    
employee_name       	string              	                    
bonus_percentage    	int                 	                    
yop                 	map<string,string>  	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
Owner:              	hdu                 	 
CreateTime:         	Fri Jul 09 00:20:27 CEST 2021	 
LastAccessTime:     	UNKNOWN             	 
Retention:          	0                   	 
Location:           	hdfs://m1:9000/user/hive/warehouse/salarydata1	 
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	numFiles            	0                   
	numRows             	0                   
	rawDataSize         	0                   
	totalSize           	0                   
	transient_lastDdlTime	1625782827          
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	field.delim         	,                   
	mapkey.delim        	:                   
	serialization.format	,                   
Time taken: 0.17 seconds, Fetched: 35 row(s)

#Load data into table from hdfs
hive> load data inpath '/salary/salarydet.txt' overwrite into table salarydata1;
Loading data to table default.salarydata1
OK
Time taken: 0.602 seconds

#test if data was loaded
hive> select * from salarydata1 limit 2;
OK
10	smith	10	{"1styear":"21000"}
20	peter	10	{"1styear":"410000"}
Time taken: 0.74 seconds, Fetched: 2 row(s)

#set configuration to print headers of your table
hive> set hive.cli.print.header=true;
hive> select * from salarydata1 limit 2;
OK
salarydata1.employee_id	salarydata1.employee_name	salarydata1.bonus_percentage	salarydata1.yop
10	smith	10	{"1styear":"21000"}
20	peter	10	{"1styear":"410000"}
Time taken: 0.125 seconds, Fetched: 2 row(s)


#check now if ur hdfs directory still has file ( shouldn't have, as data loaded from hdfs into hive table , which moves the file)
hive> !hdfs dfs -ls /salary;

#similary check your default hive location and if file exists there

hive>!hdfs dfs -ls -R /user/hive/warehouse;
drwxr-xr-x   - hdu supergroup          0 2021-07-09 00:00 /user/hive/warehouse/salarydata1
-rwxr-xr-x   2 hdu supergroup        408 2021-07-08 23:57 /user/hive/warehouse/salarydata1/salarydet.txt

#start querying
1.hive> select employee_id,employee_name,bonus_percentage,yop,yop["1styear"] as salary from salarydata1;
OK
employee_id	employee_name	bonus_percentage	yop	salary
10	smith	10	{"1styear":"21000"}	21000
20	peter	10	{"1styear":"410000"}	410000
30	marie	10	{"1styear":"551000"}	551000
40	aj	10	{"1styear":"610000"}	610000
50	daj	10	{"1styear":"710000"}	710000
60	john	10	{"2ndyear":"10000"}	NULL
70	julie	10	{"1styear":"310000"}	310000
80	july	10	{"1styear":"122000"}	122000
90	nicole	10	{"1styear":"110000"}	110000
100	amar	10	{"3rdyear":"123000"}	NULL
101	angela	10	{"1styear":"109000"}	109000
102	mark	10	{"1styear":"110000"}	110000
103	marc	10	{"1styear":"110000"}	110000
104	cool_guy	10	{"1styear":"120000"}	120000
105	stepie	10	{"1styear":"10000"}	10000
Time taken: 0.14 seconds, Fetched: 15 row(s)

2.hive> select employee_id,employee_name,bonus_percentage,yop,yop["1styear"] as salary from salarydata1 order by yop["1styear"] desc ;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = hdu_20210709002446_56b5432f-65be-4d5e-b4b0-7ea649273334
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625750270489_0005, Tracking URL = http://m2:8088/proxy/application_1625750270489_0005/
Kill Command = /usr/local/hadoop-2.7.2/bin/hadoop job  -kill job_1625750270489_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-09 00:25:02,962 Stage-1 map = 0%,  reduce = 0%
2021-07-09 00:25:10,527 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.82 sec
2021-07-09 00:25:15,911 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.12 sec
MapReduce Total cumulative CPU time: 2 seconds 120 msec
Ended Job = job_1625750270489_0005
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.12 sec   HDFS Read: 9457 HDFS Write: 766 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 120 msec
OK
employee_id	employee_name	bonus_percentage	yop	salary
50	daj	10	{"1styear":"710000"}	710000
40	aj	10	{"1styear":"610000"}	610000
30	marie	10	{"1styear":"551000"}	551000
20	peter	10	{"1styear":"410000"}	410000
70	julie	10	{"1styear":"310000"}	310000
10	smith	10	{"1styear":"21000"}	21000
80	july	10	{"1styear":"122000"}	122000
104	cool_guy	10	{"1styear":"120000"}	120000
90	nicole	10	{"1styear":"110000"}	110000
102	mark	10	{"1styear":"110000"}	110000
103	marc	10	{"1styear":"110000"}	110000
101	angela	10	{"1styear":"109000"}	109000
105	stepie	10	{"1styear":"10000"}	10000
100	amar	10	{"3rdyear":"123000"}	NULL
60	john	10	{"2ndyear":"10000"}	NULL
Time taken: 31.906 seconds, Fetched: 15 row(s)

2.Option 2:
hive> select employee_id,employee_name,bonus_percentage,yop,yop["1styear"] as salary from salarydata1 order by salary desc ;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = hdu_20210709002739_2db95d54-a1d3-41f8-8ac9-12c70b7593c1
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625750270489_0006, Tracking URL = http://m2:8088/proxy/application_1625750270489_0006/
Kill Command = /usr/local/hadoop-2.7.2/bin/hadoop job  -kill job_1625750270489_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-09 00:27:55,045 Stage-1 map = 0%,  reduce = 0%
2021-07-09 00:28:07,732 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 8.27 sec
2021-07-09 00:28:15,438 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.08 sec
MapReduce Total cumulative CPU time: 11 seconds 80 msec
Ended Job = job_1625750270489_0006
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 11.08 sec   HDFS Read: 9456 HDFS Write: 766 SUCCESS
Total MapReduce CPU Time Spent: 11 seconds 80 msec
OK
employee_id	employee_name	bonus_percentage	yop	salary
50	daj	10	{"1styear":"710000"}	710000
40	aj	10	{"1styear":"610000"}	610000
30	marie	10	{"1styear":"551000"}	551000
20	peter	10	{"1styear":"410000"}	410000
70	julie	10	{"1styear":"310000"}	310000
10	smith	10	{"1styear":"21000"}	21000
80	july	10	{"1styear":"122000"}	122000
104	cool_guy	10	{"1styear":"120000"}	120000
90	nicole	10	{"1styear":"110000"}	110000
102	mark	10	{"1styear":"110000"}	110000
103	marc	10	{"1styear":"110000"}	110000
101	angela	10	{"1styear":"109000"}	109000
105	stepie	10	{"1styear":"10000"}	10000
100	amar	10	{"3rdyear":"123000"}	NULL
60	john	10	{"2ndyear":"10000"}	NULL
Time taken: 38.667 seconds, Fetched: 15 row(s)

3.
hive> select round((bonus_percentage/100*yop["1styear"])+yop["1styear"]) as salary from salarydata1 order by salary desc;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = hdu_20210709003020_c2071916-55d2-42df-b970-cd659a264f68
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625750270489_0007, Tracking URL = http://m2:8088/proxy/application_1625750270489_0007/
Kill Command = /usr/local/hadoop-2.7.2/bin/hadoop job  -kill job_1625750270489_0007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-09 00:30:28,782 Stage-1 map = 0%,  reduce = 0%
2021-07-09 00:30:35,107 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.37 sec
2021-07-09 00:30:47,603 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.86 sec
MapReduce Total cumulative CPU time: 5 seconds 860 msec
Ended Job = job_1625750270489_0007
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.86 sec   HDFS Read: 10396 HDFS Write: 388 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 860 msec
OK
salary
781000.0
671000.0
606100.0
451000.0
341000.0
134200.0
132000.0
121000.0
121000.0
121000.0
119900.0
23100.0
11000.0
NULL
NULL
Time taken: 28.845 seconds, Fetched: 15 row(s)

4.
hive> select round(sum(round((bonus_percentage/100*yop["1styear"])+yop["1styear"]))) as salary from salarydata1 order by salary desc;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = hdu_20210709003715_d4d30f1e-29df-4475-b784-be1c9fe704f7
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625750270489_0008, Tracking URL = http://m2:8088/proxy/application_1625750270489_0008/
Kill Command = /usr/local/hadoop-2.7.2/bin/hadoop job  -kill job_1625750270489_0008
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-09 00:37:31,794 Stage-1 map = 0%,  reduce = 0%
2021-07-09 00:37:44,688 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.78 sec
2021-07-09 00:37:53,368 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 13.85 sec
MapReduce Total cumulative CPU time: 13 seconds 850 msec
Ended Job = job_1625750270489_0008
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625750270489_0009, Tracking URL = http://m2:8088/proxy/application_1625750270489_0009/
Kill Command = /usr/local/hadoop-2.7.2/bin/hadoop job  -kill job_1625750270489_0009
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2021-07-09 00:38:11,414 Stage-2 map = 0%,  reduce = 0%
2021-07-09 00:38:18,950 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.01 sec
2021-07-09 00:38:26,432 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.38 sec
MapReduce Total cumulative CPU time: 3 seconds 380 msec
Ended Job = job_1625750270489_0009
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 13.85 sec   HDFS Read: 10753 HDFS Write: 121 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.38 sec   HDFS Read: 4654 HDFS Write: 109 SUCCESS
Total MapReduce CPU Time Spent: 17 seconds 230 msec
OK
salary
3633300.0
Time taken: 72.132 seconds, Fetched: 1 row(s)

5.

#creating external table pointing to same location as your existing managed table
hive> create external table salarydata2 (employee_id int, employee_name string,bonus_percentage int,yop map<string,string>) 
    > row format delimited
    > fields terminated by ','
    > map keys terminated by ':'
    > location '/user/hive/warehouse/salarydata1';
OK
Time taken: 0.137 seconds

#querying external table
hive> select * from salarydata2;
OK
salarydata2.employee_id	salarydata2.employee_name	salarydata2.bonus_percentage	salarydata2.yop
10	smith	10	{"1styear":"21000"}
20	peter	10	{"1styear":"410000"}
30	marie	10	{"1styear":"551000"}
40	aj	10	{"1styear":"610000"}
50	daj	10	{"1styear":"710000"}
60	john	10	{"2ndyear":"10000"}
70	julie	10	{"1styear":"310000"}
80	july	10	{"1styear":"122000"}
90	nicole	10	{"1styear":"110000"}
100	amar	10	{"3rdyear":"123000"}
101	angela	10	{"1styear":"109000"}
102	mark	10	{"1styear":"110000"}
103	marc	10	{"1styear":"110000"}
104	cool_guy	10	{"1styear":"120000"}
105	stepie	10	{"1styear":"10000"}
Time taken: 0.125 seconds, Fetched: 15 row(s)

hive>drop table salarydata2;

#from PySpark
>pyspark

>>> spark.sql('select * from salarydata1')
DataFrame[employee_id: int, employee_name: string, bonus_percentage: int, yop: map<string,string>]

>>> spark.sql('select * from salarydata1').show()
+-----------+-------------+----------------+-------------------+                
|employee_id|employee_name|bonus_percentage|                yop|
+-----------+-------------+----------------+-------------------+
|         10|        smith|              10| [1styear -> 21000]|
|         20|        peter|              10|[1styear -> 410000]|
|         30|        marie|              10|[1styear -> 551000]|
|         40|           aj|              10|[1styear -> 610000]|
|         50|          daj|              10|[1styear -> 710000]|
|         60|         john|              10| [2ndyear -> 10000]|
|         70|        julie|              10|[1styear -> 310000]|
|         80|         july|              10|[1styear -> 122000]|
|         90|       nicole|              10|[1styear -> 110000]|
|        100|         amar|              10|[3rdyear -> 123000]|
|        101|       angela|              10|[1styear -> 109000]|
|        102|         mark|              10|[1styear -> 110000]|
|        103|         marc|              10|[1styear -> 110000]|
|        104|     cool_guy|              10|[1styear -> 120000]|
|        105|       stepie|              10| [1styear -> 10000]|
+-----------+-------------+----------------+-------------------+

>>> df = spark.sql('select employee_id,employee_name,bonus_percentage,yop,yop["1styear"] as salary from salarydata1 order by salary desc')

>>> df.write.format('json').save('mydata')

>>> df.write.save('mydata-parq')

#checking in hdfs if data was written
#the files were written in /user/hdu/mydata* folders as '.save' option did mention only directory name and not path on hdfs
#thus the files were written in '/user/hdu' directory for the user 'hdu'
drwxr-xr-x   - hdu supergroup          0 2021-07-09 00:56 /user/hdu/mydata
drwxr-xr-x   - hdu supergroup          0 2021-07-09 00:58 /user/hdu/mydata-parq
hdu@m1:~$ hdfs dfs -ls /user/hdu/mydata
Found 13 items
-rw-r--r--   2 hdu supergroup          0 2021-07-09 00:56 /user/hdu/mydata/_SUCCESS
-rw-r--r--   2 hdu supergroup        108 2021-07-09 00:56 /user/hdu/mydata/part-00000-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        107 2021-07-09 00:56 /user/hdu/mydata/part-00001-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        110 2021-07-09 00:56 /user/hdu/mydata/part-00002-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        110 2021-07-09 00:56 /user/hdu/mydata/part-00003-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        110 2021-07-09 00:56 /user/hdu/mydata/part-00004-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        108 2021-07-09 00:56 /user/hdu/mydata/part-00005-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        109 2021-07-09 00:56 /user/hdu/mydata/part-00006-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        114 2021-07-09 00:56 /user/hdu/mydata/part-00007-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        331 2021-07-09 00:56 /user/hdu/mydata/part-00008-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        112 2021-07-09 00:56 /user/hdu/mydata/part-00009-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        110 2021-07-09 00:56 /user/hdu/mydata/part-00010-69704592-6576-49a4-8042-191dd2e981be-c000.json
-rw-r--r--   2 hdu supergroup        182 2021-07-09 00:56 /user/hdu/mydata/part-00011-69704592-6576-49a4-8042-191dd2e981be-c000.json

hdu@m1:~$ hdfs dfs -ls /user/hdu/mydata-parq
Found 13 items
-rw-r--r--   2 hdu supergroup          0 2021-07-09 00:58 /user/hdu/mydata-parq/_SUCCESS
-rw-r--r--   2 hdu supergroup       1653 2021-07-09 00:58 /user/hdu/mydata-parq/part-00000-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1644 2021-07-09 00:58 /user/hdu/mydata-parq/part-00001-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1673 2021-07-09 00:58 /user/hdu/mydata-parq/part-00002-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1673 2021-07-09 00:58 /user/hdu/mydata-parq/part-00003-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1673 2021-07-09 00:58 /user/hdu/mydata-parq/part-00004-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1656 2021-07-09 00:58 /user/hdu/mydata-parq/part-00005-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1662 2021-07-09 00:58 /user/hdu/mydata-parq/part-00006-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1700 2021-07-09 00:58 /user/hdu/mydata-parq/part-00007-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1773 2021-07-09 00:58 /user/hdu/mydata-parq/part-00008-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1682 2021-07-09 00:58 /user/hdu/mydata-parq/part-00009-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1665 2021-07-09 00:58 /user/hdu/mydata-parq/part-00010-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet
-rw-r--r--   2 hdu supergroup       1552 2021-07-09 00:58 /user/hdu/mydata-parq/part-00011-8139a89c-5039-4b79-b481-08bc5886ba11-c000.snappy.parquet

#Writing into one file instead of mutiple files with mode as overwrite
>>> df.repartition(1).write.mode('overwrite').save('mydata-parq')
>>> df.repartition(1).write.format('json').mode('overwrite').save('mydata')

hdu@m1:~$ hdfs dfs -ls /user/hdu/mydata-parq
Found 2 items
-rw-r--r--   2 hdu supergroup          0 2021-07-09 01:05 /user/hdu/mydata-parq/_SUCCESS
-rw-r--r--   2 hdu supergroup       1980 2021-07-09 01:05 /user/hdu/mydata-parq/part-00000-4a86be5c-aefe-424c-ac3d-b6c9e6518a89-c000.snappy.parquet
hdu@m1:~$ hdfs dfs -ls /user/hdu/mydata
Found 2 items
-rw-r--r--   2 hdu supergroup          0 2021-07-09 01:06 /user/hdu/mydata/_SUCCESS
-rw-r--r--   2 hdu supergroup       1611 2021-07-09 01:06 /user/hdu/mydata/part-00000-62b43389-1bea-486c-95e3-f5dcbaf40a03-c000.json

#Optional
#Reading files from hdfs location
RDD
>>> x = sc.textFile("/user/hive/warehouse/salarydata1/salarydet.txt")
>>> x.collect()
['10,smith,10,1styear:21000', '20,peter,10,1styear:410000', '30,marie,10,1styear:551000', '40,aj,10,1styear:610000', '50,daj,10,1styear:710000', '60,john,10,2ndyear:10000', '70,julie,10,1styear:310000', '80,july,10,1styear:122000', '90,nicole,10,1styear:110000', '100,amar,10,3rdyear:123000', '101,angela,10,1styear:109000', '102,mark,10,1styear:110000', '103,marc,10,1styear:110000', '104,cool_guy,10,1styear:120000', '105,stepie,10,1styear:10000']

Dataframe
#one of the ways
>>> x = spark.read.format("text").option("delimiter",",").load("/user/hive/warehouse/salarydata1/salarydet.txt")
>>> x.show()
+--------------------+                                                          
|               value|
+--------------------+
|10,smith,10,1stye...|
|20,peter,10,1stye...|
|30,marie,10,1stye...|
|40,aj,10,1styear:...|
|50,daj,10,1styear...|
|60,john,10,2ndyea...|
|70,julie,10,1stye...|
|80,july,10,1styea...|
|90,nicole,10,1sty...|
|100,amar,10,3rdye...|
|101,angela,10,1st...|
|102,mark,10,1stye...|
|103,marc,10,1stye...|
|104,cool_guy,10,1...|
|105,stepie,10,1st...|
+--------------------+

>>> x.first()
Row(value='10,smith,10,1styear:21000')

Scenario 2:
hdu@m1:~$ hdfs dfs -mkdir /sentiments
hdu@m1:~$ hdfs dfs -put Downloads/Bigdata/BigData_HESS/Evaluation_Test/sample-files-for-test/cv* /sentiments
hdu@m1:~$ hdfs dfs -ls /sentiments
Found 1 items
-rw-r--r--   2 hdu supergroup       4043 2021-07-09 01:28 /sentiments/cv000_29416.txt

>pyspark
>>> x = sc.textFile("/sentiments/cv000_29416.txt")
>>> y = x.flatMap(lambda line: line.split(" ")) \
...              .map(lambda word: (word, 1)) \
...              .reduceByKey(lambda a, b: a + b)
>>> y.collect()

>>>y.count()
354

>>> z = y.take(10)
>>> for i in z:
...     print(i)
... 
('i', 7)
('guess', 2)
('line', 1)
('movies', 1)
('like', 3)
('this', 10)
('is', 12)
('always', 1)
('make', 5)
('sure', 1)

>>> y.saveAsTextFile("result")
>>> quit()

hdu@m1:~$ hdfs dfs -ls /user/hdu/result
Found 3 items
-rw-r--r--   2 hdu supergroup          0 2021-07-09 01:38 /user/hdu/result/_SUCCESS
-rw-r--r--   2 hdu supergroup       2149 2021-07-09 01:38 /user/hdu/result/part-00000
-rw-r--r--   2 hdu supergroup       2550 2021-07-09 01:38 /user/hdu/result/part-00001

>>> y.repartition(1).saveAsTextFile("result1")
>>> quit()                                                                      
hdu@m1:~$ hdfs dfs -ls /user/hdu/result1
Found 2 items
-rw-r--r--   2 hdu supergroup          0 2021-07-09 01:42 /user/hdu/result1/_SUCCESS
-rw-r--r--   2 hdu supergroup       4699 2021-07-09 01:42 /user/hdu/result1/part-00000

Scenario 3:
hdu@m1:~$ hdfs dfs -put Downloads/Bigdata/BigData_HESS/Evaluation_Test/sample-files-for-test/Ban* /
hdu@m1:~$ hdfs dfs -ls /
Found 5 items
-rw-r--r--   2 hdu supergroup    3966262 2021-07-09 01:47 /Bank_full.csv
drwxr-xr-x   - hdu supergroup          0 2021-07-09 00:22 /salary
drwxr-xr-x   - hdu supergroup          0 2021-07-09 01:28 /sentiments
drwxrwx---   - hdu supergroup          0 2021-07-07 12:19 /tmp
drwxr-xr-x   - hdu supergroup          0 2021-07-08 23:41 /user


